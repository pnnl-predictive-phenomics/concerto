/people/kimd999/.local/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.56it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  8.06it/s]
/people/kimd999/.local/lib/python3.12/site-packages/torch/export/_unlift.py:58: UserWarning: Attempted to insert a get_attr Node with no underlying reference in the owning GraphModule! Call GraphModule.add_submodule to add the necessary submodule, GraphModule.add_parameter to add the necessary Parameter, or nn.Module.register_buffer to add the necessary buffer
  getattr_node = gm.graph.get_attr(lifted_node)
/people/kimd999/.local/lib/python3.12/site-packages/torch/fx/graph.py:1460: UserWarning: Node _lifted_tensor_constant0_1 target _lifted_tensor_constant0 _lifted_tensor_constant0 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
/people/kimd999/.local/lib/python3.12/site-packages/torch/fx/graph.py:1460: UserWarning: Node _lifted_tensor_constant1_1 target _lifted_tensor_constant1 _lifted_tensor_constant1 of  does not reference an nn.Module, nn.Parameter, or buffer, which is what 'get_attr' Nodes typically target
  warnings.warn(f'Node {node} target {node.target} {atom} of {seen_qualname} does '
Traceback (most recent call last):
  File "/qfs/projects/protein_design/model/Chai-1/predict_all_100_designs/folder_sbatch_files/../folder_new_predict_structure_py/RFDiffusion_AA_model_9_all_sequences_after_LigandMPNN_9_Gb2_Conformer3D_COMPOUND_CID_6424189_predict_structure.py", line 34, in <module>
    output_pdb_paths = run_inference(
                       ^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/bin/Miniconda3-py3.12.4_Linux/lib/python3.12/site-packages/chai_lab/chai1.py", line 274, in run_inference
    output_cif_paths, _, _, _ = run_folding_on_context(
                                ^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/bin/Miniconda3-py3.12.4_Linux/lib/python3.12/site-packages/chai_lab/chai1.py", line 387, in run_folding_on_context
    trunk = load_exported(f"{model_size}/trunk.pt2", device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/bin/Miniconda3-py3.12.4_Linux/lib/python3.12/site-packages/chai_lab/chai1.py", line 92, in load_exported
    exported_program = torch.export.load(local_path)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/export/__init__.py", line 299, in load
    return load(
           ^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/_export/__init__.py", line 304, in load
    ep = deserialize(artifact, expected_opset_version)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/_export/serde/serialize.py", line 1999, in deserialize
    .deserialize(
     ^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/_export/serde/serialize.py", line 1829, in deserialize
    .deserialize(
     ^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/_export/serde/serialize.py", line 1510, in deserialize
    state_dict=deserialize_torch_artifact(serialized_state_dict),
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/_export/serde/serialize.py", line 283, in deserialize_torch_artifact
    artifact = torch.load(buffer)
               ^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/serialization.py", line 1025, in load
    return _load(opened_zipfile,
           ^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/serialization.py", line 1446, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/serialization.py", line 1416, in persistent_load
    typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/serialization.py", line 1390, in load_tensor
    wrap_storage=restore_location(storage, location),
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/serialization.py", line 390, in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/serialization.py", line 270, in _cuda_deserialize
    return obj.cuda(device)
           ^^^^^^^^^^^^^^^^
  File "/people/kimd999/.local/lib/python3.12/site-packages/torch/_utils.py", line 114, in _cuda
    untyped_storage = torch.UntypedStorage(
                      ^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 
